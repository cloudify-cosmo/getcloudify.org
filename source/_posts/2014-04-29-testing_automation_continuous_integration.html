---
layout: blogpost
title: From 0 to 10000X - Testing Automation + Continuous Integration for Any Environment
image: kobik.jpg
author: Kobi Kisos
tags: 
 - Test Automation
 - Continuous Integration
 - Programming 
 - Cloud Orchestration 
 - Multi-Cloud
---
<notextile>
<img src="/img/blog/testautomation.jpg" alt="Test Automation + Continuous Integration">
<br/>
<br/>

<p><h3><strong>How we built a testing-as-service infrastructure for our team of developers using Git, QuickBuild, JClouds, Cloudify, Logstash, TestNG, Tomcat, &amp; MySQL</strong></h3><p>

<p>System testing is one of the most tedious, time consuming and error prone procedures in software development. More so when developing and testing distributed middleware products. All this, when achieving a robust continuous integration system is highly reliant on a completely automated testing environment.&#160; </p>

<p>In this post I’m going to take a deep dive on the testing automation used by our developers to test even their most complicated code in fully distributed production like environment, running more than 10K tests a day. Using this framework, our developers can test their code at scale (in some cases up to hundreds of nodes) at the click of a button, across AWS, RackSpace, HP OpenStack and our-prem environment.</p>

<h3><strong>Testing Automation as Our Continuous Integration Enabler</strong></h3>

<p>One of the key factors that enable continuous integration is automatic testing. If you want to achieve very complicated testing you will need to build automatic tests that can run all the time in order to get quick feedback. This is eventually the main goal in continuous integration - a quick feedback loop. </p>

<p>In order to achieve this level of testing we chose to run our tests in the cloud using Cloudify.&#160; This is actually kind of like drinking our own merlot - using our products in production to test their production readiness.&#160; The actual use of the product when putting it to the test actually affords us even further testing of its capabilities.&#160; (But that’s just an added bonus).</p>

<p>Using Cloudify for the purpose of testing automation is really not very different from running any app in the cloud. To do this, we wrote a service which knows how to run our tests, and this service uses the testing framework, TestNG. We chose TestNG, because this is a testing framework our developers are very familiar with, as it is a unit test testing framework, which enables a lower barrier of entry.&#160; This all ties into our efforts of tool unification and developer portability.&#160; This also enables our developers to run and debug the tests within the IDE on their own laptop, further simplifying the complexity normally involved with system testing.</p>

<p>Using Cloudify enables us to install many of the same service instances in parallel and in this way we can run our spectrum of tests in parallel, as well. So, for example, if we have 1000 tests to run - we can distribute them among&#160; our 100 service instances, where each service instance runs only 10 tests. </p>

<hr>

<span class="pullquote-left">
  <h2><font style="font-weight: bold" size="5" face="Baskerville Old Face"><em>Test drive Cloudify and build your testing-as-a-service framework in minutes.</em></font>&nbsp; &nbsp; <a class="btn btn-large btn-theme btn-rounded" href="{{ site.baseurl }}/downloads/get_cloudify.html" target="_blank"><i class="icon-plus"></i> Go </a></h2></span>
  
  <hr>

<p>Just as a side note, these are all system tests, not unit tests, where by default the system is distributed across a number of machines that contain the many different integrated software components - so each test actually tests all of these software components.</p>

<p>This parallel system allows us to run a few suites simultaneously. Each suite is a logical group that tests a different feature, or different set of features. Each suite is divided into a few service instances. This makes it possible to run a larger number of tests in parallel.&#160; This is our method for <a href="https://github.com/CloudifySource/Cloudify-iTests-Deployer" target="_blank"> testing automation</a>, and it’s all available in GitHub.</p>

<h3><strong>How We Do It</strong></h3>

<p>Our build servers are on all the time and they build, compile and package our products. </p>

<p>We also have two additional dedicated machines for the testing framework on EC2 with elasticsearch, logstash, and Redis, and&#160; the other machine is Tomcat and MySQL. We also have a third machine that is the Cloudify Manager, that orchestrates the testing services.</p>

<p>When this is completed the testing begins.&#160; What the build server does is install the testing services.&#160; After these are installed, Cloudify starts the machine, and then the service(s) or service instances, and at this point the test clients start to run. </p>

<p>There are a few possible test flows and scenarios. The first, is running a test on a single machine - that is the client and the server run on the same machine, and in our case also on the same cloud - this is the simplest one. </p>

<img src="/images/ITest Management.png" width="500" "align-text:center">

<p>A more complicated scenario is running the client tests on one cloud and the tested service on any other cloud. For example when we want to test our HP cloud driver, we run the client service on EC2, and start service on other clouds, in this case HP Cloud. This is a pretty complicated flow.</p>

<img src="/images/Cross-Cloud Tests.png" width="600" "align-text:center">

<p>In this way we test other clouds like HP Cloud, Rackspace, Exoscale, EC2, Softlayer, and Softlayer bare metal.&#160; This does not include testing of Cloudify’s BYON functionality, this is tested in our own local lab.</p>

<p>All the logs are then sent to the logstash server, so we can have all of the runtime logs and <a href="{{ site.baseurl }}/2014/04/17/logging_python_devops_tools.html" target="_blank">use them as needed</a>.</p>

<p>When all of the test instances or services instances have finished running their tests,&#160; we send the team the results that contain the logs from the logstash server, and we also keep the results in MySQL. We have a dashboard that you see in our SCRUM room with all the suites results, so our team is always up to date with the latest results. </p>

<p>Just as an aside, apropos our previous post on <a href="{{ site.baseurl }}/2014/02/23/behavior_driven_development_and_user_experience.html" target="_blank">test driven development</a>, (AKA Behavior Driven Development) this level of testing differs, as it deals with the entire system.&#160; TDD deals with unit tests - and just for context, these unit tests run every half hour for each product/component.&#160; </p>

<h3><strong>Run: Success</strong></h3>

<p>These testing processes have helped us greatly improve our developer’s code. In the last two years, we have rewritten our application from scratch - which is a very complicated undertaking. Since we work in agile scrum, we have a two week sprint. Each day the developer sees the dashboard and if its a regression result they fix it immediately and at the end of the sprint, everything new feature is considered beta, and every other bug is a “show stopper.” </p>

<p>Eight years ago all our tests ran once or twice in a release.&#160; Five years ago - once a day.</p>

<p>Today all our tests run pretty much all the time which allows for the quick feedback we need for our continuous integration processes and bug fixes - which lends to better code, which is the ultimate goal eventually.</p>

</notextile>
