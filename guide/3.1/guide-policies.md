---
layout: bt_wiki
title: Policies Guide
category: Guides
publish: true
abstract: "A guide to Cloudify Policies"
pageord: 500

types_yaml_link: http://www.getcloudify.org/spec/cloudify/3.1/types.yaml
diamond_plugin_ref: plugin-diamond.html
dsl_groups_spec: dsl-spec-groups.html
workflows_dsl_spec: dsl-spec-workflows.html
diamond_package_ref: https://github.com/BrightcoveOS/Diamond
---

{%summary%}This guide explains what policies are and how to use them{%endsummary%}


# Introduction to Policies

Policies provide a way of analyzing a stream of events that correspond to a group of nodes (and their instances).
The analysis process occurs in real time and enables triggering actions based on its outcome.

The stream of events for each node instance is read from a deployment specific RabbitMQ queue by the policy engine ([Riemann](http://riemann.io/)). It may come from any source but is typically generated by a monitoring agent installed the node's host. (See: [Diamond Plugin]({{page.diamond_plugin_ref}})).

The analysis is performed using [Riemann](http://riemann.io/). Cloudify starts a Riemann core for each deployment with its own Riemann configuration. The Riemann configuration is generated based on the blueprint's `groups` section that will be described later in this guide.

A part of the API that Cloudify provides on top of Riemann, allows activating policy triggers. Usually, that simply means executing a workflow in response to some state (e.g. The tomcat CPU usage was above 90% for the last five minutes).
That workflow may increase the number of node instances, for example, to handle an increased load.

The policies logic itself is written in [Clojure](http://clojure.org/) using Riemann's API. Cloudify adds a thin API layer on top of these API's.

# Using Policies

Policies are configured in the `groups` section of a blueprint.
An example:

{% highlight yaml %}
groups:
  # arbitrary group name
  my_group:

    # all nodes this group applies to
    # in this case, assume node_vm and mongo_vm
    # were defined in the node_templates section.
    # All events that belong to node instances of
    # these nodes will be processed by all policies specified
    # within the group
    members: [node_vm, mongo_vm]

    # each group specifies a set of policies
    policies:

      # arbitrary policy name
      my_policy:

        # the policy type to use. Here we use one
        # of the built-in policy types that identifies
        # host failures based on a keep alive mechanism
        type: cloudify.policies.types.host_failure

        # policy specific configuration
        properties:
          # Not really a host_failure property, only serves as an example.
          some_prop: some_value

        # This section specifies what should be
        # triggered when the policy is "triggered"
        # (more than one trigger can be specified)
        triggers:
          # arbitrary trigger name
          execute_autoheal_workflow:

            # using the built-in 'execute_workflow' trigger
            type: cloudify.policies.triggers.execute_workflow

            # trigger specific configuration
            parameters:
              workflow: auto_heal
              workflow_parameters:
                # the failing node instance id is exposed by the
                # host_failure policy on the event that triggered the
                # execute workflow trigger. Access to properties on
                # that event are as demonstrated below
                node_id: { get_property: [SELF, node_id] }

{% endhighlight %}

# Built-in Policies

Cloudify comes with a number of built-in policies.

Built-in policies are declared in [`types.yaml`]({{page.types_yaml_link}}), which is usually imported either directly or indirectly via other imports.

{% highlight yaml %}
# snippet from types.yaml
policy_types: {}
    # TODO

{% endhighlight %}

Built-in policies are not special in any way - they use the same API any other custom policy is able to use.

# Writing a Custom Policy

Advanced users may wish to write custom policies.
To learn how to write a custom policy, refer to the [policies authoring guide](guide-authoring-policies.html).

<!---
# Auto Healing

Auto healing is the process of automatically identifying when a certain component of your system is failing and fixing the system state without any user intervention.

Cloudify comes with built-in support for auto healing different components.

There are several things that need to be configured to work with auto healing. This guide will walk you through them, and explain the limitations the mechanism currently has.

In short, the process involves:

* Configuring monitoring on compute nodes that should have auto healing.
* Adding the `autoheal` workflow to the blueprint.
* Configuring the `groups` section with appropriate policy types and triggers.

{%warning title=Limitations%}
Make sure you don't miss the [Limitations](#limitations) section.
{%endwarning%}

## Monitoring

Add monitoring to compute nodes that require auto healing.

At the moment, due to a limitation of existing policies, it is required that exactly one metric will be published by the monitored nodes.

Another limitation requires that only compute nodes will have auto heal configured for them. Otherwise, when a certain compute node instance fails, node instances that are contained in it are also likely to fail, which in turn will cause the auto heal workflow to be triggered multiple times.

We will show an example using the built-in [Diamond]({{page.diamond_package_ref}}) support that comes with Cloudify. Please refer to the [Diamond Plugin]({{page.diamond_plugin_ref}}) documentation for further details on configuring diamond based monitoring.

{% highlight yaml %}
node_templates:
  some_vm:
    interfaces:
      cloudify.interfaces.monitoring:
        start:
          implementation: diamond.diamond_agent.tasks.add_collectors
          inputs:
            collectors_config:
              ExampleCollector: {}
        stop:
          implementation: diamond.diamond_agent.tasks.del_collectors
          inputs:
            collectors_config:
              ExampleCollector: {}
{% endhighlight %}

Notice how we use the `ExampleCollector` that comes with Diamond. We do this because of the policy limitation described above. Most built-in collectors in Diamond generate more than one metric, which may cause the auto heal workflow to be triggered more than once. The `ExampleCollector` generates a single metric whose value is consistently `42`. We will use this collector as a keep alive collector combined with the `host_failure` policy that will be described below.

Future versions of Cloudify will address this limitation by allowing a specific metric regex to be specified when configuring the policy.

## [Workflow]({{page.workflows_dsl_spec}}) Configuration

The auto heal workflow will execute a process of tasks that is similar in nature to calling the `uninstall` workflow and then the `install` workflow. The main difference, is that this process is only executed on the subgraph of node instances that are contained in the failing compute node instance and the relationships these node instances have with other node instances.

At the moment, the auto heal workflow that comes with Cloudify is not configured in our `types.yaml`. Until this workflow is added to the built-in workflows, add the following to the blueprint:

{% highlight yaml %}
workflows:
  autoheal:
    mapping: default_workflows.cloudify.plugins.workflows.auto_heal_reinstall_node_subgraph
    parameters:
      node_id: {}
      diagnose_value: {}
{% endhighlight %}

## [Groups]({{page.dsl_groups_spec}}) Configuration

After we have monitoring set up, and the autoheal workflow configured, it's time to plug things together.

This example contains several inline comments, so make sure you read them to have better understanding of
how things work.

{% highlight yaml %}
groups:
  some_vm_group:
    # adding the some_vm node template that was previously configured
    members: [some_vm]
    policies:
      host_failure_policy:
        # using the 'host_failure' policy type
        type: cloudify.policies.host_failure
        triggers:
          autoheal_trigger:
            # using the 'execute_workflow' policy trigger
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              # configuring this trigger to execute the autoheal workflow
              # that was previously configured
              workflow: autoheal

              # The autoheal workflow will get
              # its parameters from the event that triggered
              # its execution
              workflow_parameters:
                # The host_failure policy adds 'failing_node' to
                # the event before processing its triggers.
                # the 'faling_node' will be the node instance id
                # of the node that failed. In our case, it will be
                # something like 'some_vm_afd34'
                node_id: { get_property: [SELF, failing_node] }

                # Contextual information added by the triggering policy
                diagnose_value: { get_property: [SELF, diagnose] }
{% endhighlight %}

In this example, we configured a group consisting of the `some_vm` node that was previously configured.

We then configured a single `host_failure` policy for this group. The `host_failure` policy works by checking for expired events. The first monitoring event sent by a certain node instance will add this event type to the policy engine index (Riemann's index mechanism). After which, if this type of event was not sent for a period of 60 seconds for a certain node instance, it's considered a failed node instance and the policy triggers are executed.

The policy in the example has one `execute_workflow` policy trigger configured, which is mapped to execute the `autoheal` workflow.

## Limitations

Currently, there are several limitations to using auto healing. They are all desribed below and throughout the auto healing section above.

It should be noted that these limitations are something we are currently in the process of ironing out. Most of them (perhaps all) will be fixed by the time the next version of Cloudify is released.

{%warning title=Limitations%}
* Only compute nodes can have auto healing configured for them.
* Monitoring on these compute nodes can only generate a single metric type.
* Each compute node must be specified in its own group in the `groups.members` section
* Monitoring is set up when the deployment is created. As such, after the `uninstall` workflow has been called on a deployment, the `autoheal` workflow will probably be triggered after a while. At the moment, this is resolved by having the auto heal workflow check the state of the faling node host. If its state is different than `started` the workflow will end its execution without actually doing anything. This means that when the deployment executions are listed, you may find unexpected `autoheal` workflow executions in `terminated` state.
{%endwarning%}
-->