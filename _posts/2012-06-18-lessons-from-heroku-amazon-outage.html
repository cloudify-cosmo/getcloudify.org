---
layout: blogpost
title: Lessons From The Heroku Amazon Outage
image: nati.png
author: Nati Shalom
tags: 
 - PaaS
 - Open PaaS
 - Heroku
 - AWS 
 - EC2 
---


<div xmlns="http://www.w3.org/1999/xhtml">
	<p>Earlier this week we experienced the third significant AWS outage in the past 14 months, as reported by <a href="http://www.thewhir.com/profile/justinlee">Justin Lee</a> in his report&nbsp;<a href="http://www.thewhir.com/web-hosting-news/heroku-pinterest-among-sites-knocked-offline-in-amazon-data-center-outage">Heroku, Pinterest Among Sites Knocked Offline in Amazon Data Center Outage</a>&nbsp;on theWhirr magazine:</p>
	<blockquote>
		<p>An&nbsp;<a href="http://aws.amazon.com/">Amazon</a>&nbsp;data center in Ashburn, Virginia suffered a power outage at 9:45 p.m. PDT on Thursday, causing some websites using AWS cloud technology to go offline.&nbsp;High-profile websites like Heroku, Pinterest, Quora and HootSuite saw downtime, as well as many smaller sites.</p>
	</blockquote>
	<p>In this post I&#39;d like to briefly address the lessons from this experience, and more importantly, focus on the lessons from this sort of failure and their effect on public PaaS offerings, such as Heroku.</p>
	<h3>Lesson from the Heroku Outage: Choose the Right PaaS for the Job</h3>
	<p>One of the promises of PaaS is&nbsp;productivity. PaaS providers like Heroku claim to increase productivity by abstracting&nbsp;the user from the details of the underlying infrastructure, and even go so far as to claim that PaaS makes application operations redundant.</p>
	<h4>Lesson 1: Choose the Right PaaS for the Job</h4>
	<p>The main lesson from this outage is that relying on the PaaS provider to carry all your operations isn&#39;t always a safe bet. When we move to PaaS we still need to understand how they run their disaster recovery, high-availability, and scaling procedures. Heroku-like&nbsp;PaaS also forces you to a lowest common denominator approach to dealing with continuous availability&nbsp;and scalability. In reality, however, there are many tradeoffs between&nbsp;scalability, performance, and high&nbsp;availability. The best fit between those tradeoffs tends to be application specific, so compromising on a lowest common denominator could be less&nbsp;productive&nbsp;and more costly at the end of the day.</p>
	<p>Which brings me to the last point in this section -- PaaS was meant to provide a higher productivity for running our apps on the cloud by abstracting the details of how we run our application (the operation) from the application developer. The black-box approach of many of the&nbsp;public&nbsp;PaaS offerings, such as Heroku, is often an extreme measure in this regard. There is often a close coupling between what the application does and the way we run it. A new class of Open PaaS&nbsp;platforms&nbsp;such as <a href="http://feedproxy.google.com/~r/NatiShalom/cloudifysource.org">Cloudify</a>, Cloud Foundry, and OpenShift offer a different open source alternative that gives you more control of the underlying PaaS&nbsp;platform. Cloudify takes it even further, providing an open&nbsp;recipe&nbsp;model that integrates with the likes of Chef, enabling you to easily customize and control your operations without affecting developer&nbsp;productivity.</p>
	<h4>Lesson 2: Database Availability Must Address Data Center Failure</h4>
	<p>The other area that Heroku, and to be honest, most other PaaS offering don&#39;t adequately address is database&nbsp;high-availability, which is obviously a tough area. Specificaly, in the event of data center failure or availability zone failure, as in the present case. To deal with database availability, it is necessary to ensure real-time synchronization of the database across sites. The example at the bottom of this post refers to a specific way this can be done, between two mySql instances running on Amazon and Rackspace with Cloudify.</p>
	<h3>General Lessons:</h3>
	<h4>Lesson 3: Coping with Failure, Avoiding a Single Point of Failure</h4>
	<p>The general lesson from this and previous failures is actually not new. To be fair, this lesson is not specific to AWS or to any cloud service. Failures are inevitable, and often happen when and where we least expect them to.&nbsp;Instead of trying to prevent failure from happening we should design our systems to cope with failure.&nbsp;<br />
		The method of dealing with failures is also not that new -- use redundancy, don&#39;t rely on a single point failure (including a data center or even a data&nbsp;center&nbsp;provider). Automate the fail-over process, etc...</p>
	<h3>Haven&#39;t Learned from Past Lessons?</h3>
	<p>The question that comes out of this experience IMO is not&nbsp;necessarily&nbsp;how to deal with failures (those lessons are as old as the mainframe or even older), but rather -- why are we failing to implement the lessons? Assuming that the people running these systems are among the best in the industry makes this question even more&nbsp;intresting. Here is my take on it:</p>
	<ul>
		<li><strong>We give up&nbsp;responsibility&nbsp;when we move to the cloud</strong>:&nbsp;When we move our operations to the cloud, we often assume that we&#39;re&nbsp;out-sourcing&nbsp;our data center operation&nbsp;completely, including our disaster recovery procedures. The truth is that when we move to the cloud we&#39;re only outsourcing the infrastructure, not our operations, and the&nbsp;responsibility&nbsp;of how to use this infrastructure remain ours.&nbsp;</li>
		<li><strong>Complexity</strong>:&nbsp;The current DR processes and tools were designed for a pre-cloud world, and do not work well in a dynamic environment, such as the cloud. Many of the tools that are provided by the cloud vendors (Amazon in this&nbsp;specific&nbsp;case) are still fairly complex to use.</li>
	</ul>
	<h3>Implementing Past Lessons&nbsp;in a Cloud World</h3>
	<p>The first point is is easy -- we need to assume full&nbsp;responsibility&nbsp;for our applications&#39; disaster recovery procedures, in the cloud world just as if we were running our own data center. The hard part in the cloud world is that we often have less&nbsp;visibility, control, and&nbsp;knowledge&nbsp;of the infrastructure, which affects our ability to protect our applications -- and each sub-component of our application -- from failure. On the other hand, the cloud&nbsp;enables us to spawn new instances easily on various data center locations, a.k.a&nbsp;Availability&nbsp;Zones.&nbsp;</p>
	<p>And so, most failures can be addressed by moving from the failed system to a completely different system regardless of the root cause of the failure. Therefore, the first lesson is that in the cloud world it is easier to implement disaster recovery plans, by moving our application traffic to a completely different&nbsp;redundant&nbsp;system in a snap, rather than trying to protect every component of our application from a failure. If we&#39;re willing to tolerate a short window of downtime, we can even use an on-demand backup site rather than pay the consistent cost and overhead of maintaining a hot backup site.</p>
	<p>Which brings me to the next point: What do we need to build such a solution?</p>
	<p>A consistent&nbsp;redundant environment that is ready to take over in case of failure needs to include the following elements:</p>
	<ul>
		<li><strong>Workload Migratio</strong>n: Specifically, the ability to clone your application environment and configuration in a consistent way accorss sites, and on demand.</li>
		<li><strong>Data Synchronization</strong>: The ability to maintain a real-time copy of the data between two sites.&nbsp;</li>
		<li><strong>Network Connectivity</strong>: Enabling the flow of network traffic between two sites.</li>
	</ul>
	<p>Which leads to the second&nbsp;challenge:&nbsp;Complexity. Here, I would use an example of a simple web-app and show how we can easily create two sites on demand. I would even go so far as to set this environment on two&nbsp;separate&nbsp;clouds to show how we can ensure an even higher degree of redundancy by running our application across two different cloud providers.</p>
	<h3>A Step by Step Example: Fail-Over from AWS to Rackspace</h3>
	<p>In this example, we picked Amazon and Rackspace as the two target sites. The same solution would also&nbsp;work between two&nbsp;availability&nbsp;zones in Amazon or data centers.&nbsp;We&#39;ve&nbsp;also tried the same example with a combination of HP Cloud Services and a flavor of a <a href="/guide/2.1/setup/configuring_byon.html">private cloud</a>.</p>
	<p>The example demonstrates a very simple web application with global load-balancer (Rackspace), and a Web application (Pet Clinic) based on Tomcat as the Web front end and MySQL as the database.</p>
	<p><img alt="" src="http://horovits.files.wordpress.com/2012/06/cloud-burst-arch.jpg?w=500&amp;h=375&amp;__SQUARESPACE_CACHEVERSION=1339711730747" /></p>
	<p>On both ends we used&nbsp;<a href="http://www.gigaspaces.com/wiki/display/XAP8/Multi-Site+Replication+over+the+WAN">GigaSpaces XAP Transactional WAN replication</a>&nbsp;as a replication channel between the two instances of MySQL and&nbsp;<a href="http://cloudifysource.org/">Cloudify</a>&nbsp;to handle the workload migration between the sites.</p>
	<p>The Goal:&nbsp;Fail-over with no change to the target application</p>
	<p>The goals that we set for ourselves were seamlessness and no change to the target application or database. We achieved this by plugging the replication service into the existing instances of MySQL. The replicating service listened to the MySQL events and replicated every change to its peer MySQL instance. Cloudify enabled us to clone the same application in both Amazon and Racksapce while maintaining a consistent configuration setup as well as consistent scaling and fail-over SLAs. Cloudify does this by abstracting all the information through portable recipe definitions. Cloudify wraps the application instances with its management and control services based on the&nbsp;definition provided in the&nbsp;recipe. This enabled us to clone the environment as well as add elastic scaling without changing the target application (Pet Clinic&nbsp;in this case).</p>
	<p>You can read the full details, including code references on Github, in Dotan Horvits&#39; blog post: <a href="http://horovits.wordpress.com/2012/06/17/aws-outage-thoughts-on-disaster-recovery-policies/">AWS Outage Thoughts on Disaster Recovery Policies</a>.</p>
	<!--
	<p><strong>Join GigaSpaces for our upcoming events to learn more about this topic:</strong></p>
	<ul>
		<li>Transactional Cross-Site Data Replication webinar this Wednesday June 20th (9:00 AM PST) that will teach you how to achieve consistent data replication across sites in just 10 minutes, enabling disastery recovery, cloud bursting, and more. Register <a href="http://bit.ly/IM0w9F" title="Transactional Cross-Site Data Replication">here</a>.</li>
		<li>The DevOps PaaS Infusion Meetup this Tuesday June 19th at 6:30 PM - NetFlix and OpsCode will be joining GigaSpaces at this meetup in NY where one of the highlights will be a panel on lesson learning from the recent AWS outage.&nbsp; Register <a href="http://bit.ly/MxH9bM" title="The DevOps PaaS Infusion">here</a>.</li>
	</ul>
	<p>&nbsp;</p> -->
	<p><strong>References</strong>:</p>
	<ul>
		<li><a href="http://www.thewhir.com/web-hosting-news/heroku-pinterest-among-sites-knocked-offline-in-amazon-data-center-outage">Heroku, Pinterest Among Sites Knocked Offline in Amazon Data Center Outage</a></li>
		<li><a href="http://horovits.wordpress.com/2012/06/17/aws-outage-thoughts-on-disaster-recovery-policies/">AWS Outage Thoughts on Disaster Recovery Policies</a></li>
		<li><a href="http://natishalom.typepad.com/nati_shaloms_blog/2012/05/mapping-the-cloudpaas-stack.html">Mapping the Cloud Stack</a></li>
		<li><a href="http://natishalom.typepad.com/nati_shaloms_blog/2012/05/making-cloud-bursting-a-practical-reality.html">Making Cloud Bursting a Practical Reality</a></li>
	</ul>
</div>
